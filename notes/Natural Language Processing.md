---
tags:
  - ml/dl/nlp
  - type/moc
---
Created: 2023-09-07 18:38

# Map of Content

Humans process languages in a different way that machines do. Computer cannot understand a single word in its textual form. Instead, we implement different techniques to encode/embed them into numbers before feeding them into desired architecture.

There are different methods of making the computer understand our language as explained in [[From Raw Text To Numbers]] as well as employing various techniques that align to the task at hand (for example [[Sequence models in NLP]] ). However, in 2017, a significant breakthrough happened with the introduction of [[Transformer architecture]] and it's utilization of self-attention mechanism that has pushed forward the performance of natural language models. 

Given a problem at hand and not large amounts of data to train, we can also consider [[Transfer learning]] to exploit the knowledge gained from a previous task to improve the generalization of another.

![](/img/nlp-timeline-of-nlp-models.png)

